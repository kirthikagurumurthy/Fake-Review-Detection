{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import treebank\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('deceptive-opinion.csv')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"was'nt\") in stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df,test_size):\n",
    "    if isinstance(test_size,float):\n",
    "        test_size = round(test_size * len(df))\n",
    "        \n",
    "    if(test_size > len(df)):\n",
    "        return 0, df\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices,k = test_size)\n",
    "\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    \n",
    "    return train_df, test_df\n",
    "train_df,test_df = train_test_split(dataset,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaderSentimentValue(review):\n",
    "    sid_obj =  SentimentIntensityAnalyzer()\n",
    "    \n",
    "    sentiment_dict = sid_obj.polarity_scores(review)\n",
    "    return sentiment_dict['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words_list = set(opinion_lexicon.positive())\n",
    "negative_words_list = set(opinion_lexicon.negative())\n",
    "tokenizer = treebank.TreebankWordTokenizer()\n",
    "def getSentimentValue(review):\n",
    "    senti = 0\n",
    "    words = [word.lower() for word in tokenizer.tokenize(review)]\n",
    "    for word in words:\n",
    "        if word in positive_words_list:\n",
    "            senti += 1\n",
    "        elif word in negative_words_list:\n",
    "            senti -= 1\n",
    "    return senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_vocab = {}\n",
    "notfake_vocab = {}\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lemma('bad.n.01.badness')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wn.synsets(\"bad\")[0]).lemmas()[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_vocab = {}\n",
    "notfake_vocab = {}\n",
    "for i in range(len(train_df)):\n",
    "    review = list(train_df['text'])[i]\n",
    "    words = word_tokenize(review)\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words and len(word) != 1:\n",
    "            if list(train_df['deceptive'])[i] == \"truthful\":\n",
    "                if lemmatizer.lemmatize(word.lower()) not in notfake_vocab:\n",
    "                    notfake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                else:\n",
    "                    notfake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "            elif lemmatizer.lemmatize(word.lower()) not in fake_vocab:\n",
    "                fake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "            else:\n",
    "                fake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "\n",
    "\n",
    "fake_vocab = sorted(fake_vocab.items(), key = \n",
    "         lambda kv:(-kv[1], kv[0]))   \n",
    "notfake_vocab =sorted(notfake_vocab.items(), key = \n",
    "             lambda kv:(-kv[1], kv[0]))  \n",
    "finalFakeVocab = []\n",
    "finalNotFakeVocab = []\n",
    "for i in range(150):\n",
    "    finalFakeVocab.append(fake_vocab[i][0])\n",
    "    finalNotFakeVocab.append(notfake_vocab[i][0])\n",
    "fake_vocab = finalFakeVocab\n",
    "notfake_vocab = finalNotFakeVocab\n",
    "\n",
    "old_len_fakevocab = len(fake_vocab)\n",
    "old_len_notfakevocab = len(notfake_vocab)\n",
    "new_len_fakevocab = 0;\n",
    "new_len_notfakevocab = 0;\n",
    "\n",
    "while(True):\n",
    "    old_len_fakevocab = len(fake_vocab)\n",
    "    old_len_notfakevocab = len(notfake_vocab)\n",
    "    for word in notfake_vocab:\n",
    "        for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                            for lemma in synset.lemmas():\n",
    "                                if lemma.name() not in notfake_vocab:\n",
    "                                    notfake_vocab.append(lemma.name())\n",
    "\n",
    "\n",
    "\n",
    "    for word in fake_vocab:\n",
    "        for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                            for lemma in synset.lemmas():\n",
    "                                if lemma.name() not in fake_vocab:\n",
    "                                    fake_vocab.append(lemma.name())\n",
    "\n",
    "    new_len_fakevocab = len(fake_vocab);\n",
    "    new_len_notfakevocab = len(notfake_vocab);\n",
    "\n",
    "    if(old_len_fakevocab == new_len_fakevocab and old_len_notfakevocab == new_len_notfakevocab):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['room',\n",
       " 'hotel',\n",
       " 'stay',\n",
       " \"n't\",\n",
       " 'great',\n",
       " 'would',\n",
       " 'chicago',\n",
       " 'staff',\n",
       " 'night',\n",
       " 'one',\n",
       " 'service',\n",
       " 'bed',\n",
       " 'location',\n",
       " 'stayed',\n",
       " 'u',\n",
       " 'get',\n",
       " \"'s\",\n",
       " 'time',\n",
       " '...',\n",
       " 'nice',\n",
       " 'could',\n",
       " 'desk',\n",
       " 'good',\n",
       " 'floor',\n",
       " 'bathroom',\n",
       " 'even',\n",
       " 'day',\n",
       " 'also',\n",
       " 'clean',\n",
       " 'view',\n",
       " 'like',\n",
       " 'two',\n",
       " 'front',\n",
       " 'got',\n",
       " 'back',\n",
       " 'restaurant',\n",
       " 'breakfast',\n",
       " 'lobby',\n",
       " 'friendly',\n",
       " 'go',\n",
       " 'place',\n",
       " 'small',\n",
       " 'comfortable',\n",
       " 'well',\n",
       " 'door',\n",
       " 'really',\n",
       " 'bar',\n",
       " 'first',\n",
       " 'never',\n",
       " 'free',\n",
       " 'called',\n",
       " 'michigan',\n",
       " 'area',\n",
       " 'helpful',\n",
       " 'booked',\n",
       " 'price',\n",
       " 'told',\n",
       " 'next',\n",
       " 'better',\n",
       " 'minute',\n",
       " 'recommend',\n",
       " 'guest',\n",
       " 'morning',\n",
       " 'experience',\n",
       " 'made',\n",
       " 'suite',\n",
       " 'business',\n",
       " 'elevator',\n",
       " 'much',\n",
       " '--',\n",
       " 'check',\n",
       " 'reservation',\n",
       " 'right',\n",
       " 'take',\n",
       " 'best',\n",
       " 'asked',\n",
       " 'city',\n",
       " 'call',\n",
       " 'thing',\n",
       " 'everything',\n",
       " 'many',\n",
       " 'people',\n",
       " 'problem',\n",
       " 'trip',\n",
       " 'review',\n",
       " 'coffee',\n",
       " 'rate',\n",
       " 'staying',\n",
       " 'water',\n",
       " 'every',\n",
       " 'excellent',\n",
       " 'said',\n",
       " 'say',\n",
       " 'walk',\n",
       " 'weekend',\n",
       " 'another',\n",
       " 'close',\n",
       " 'large',\n",
       " 'shower',\n",
       " 'arrived',\n",
       " 'internet',\n",
       " 'year',\n",
       " 'shopping',\n",
       " 'walking',\n",
       " 'wonderful',\n",
       " 'work',\n",
       " 'pool',\n",
       " 'wall',\n",
       " 'way',\n",
       " 'found',\n",
       " 'away',\n",
       " 'food',\n",
       " \"'ve\",\n",
       " 'charge',\n",
       " 'little',\n",
       " 'block',\n",
       " 'make',\n",
       " 'street',\n",
       " 'around',\n",
       " 'hour',\n",
       " 'need',\n",
       " 'old',\n",
       " 'still',\n",
       " 'took',\n",
       " 'went',\n",
       " 'beautiful',\n",
       " 'concierge',\n",
       " 'phone',\n",
       " 'definitely',\n",
       " 'week',\n",
       " 'tv',\n",
       " 'use',\n",
       " 'want',\n",
       " 'though',\n",
       " 'however',\n",
       " 'king',\n",
       " 'lot',\n",
       " 'mile',\n",
       " 'parking',\n",
       " 'quite',\n",
       " 'spacious',\n",
       " 'going',\n",
       " 'huge',\n",
       " 'loved',\n",
       " 'manager',\n",
       " 'open',\n",
       " 'window',\n",
       " 'checked',\n",
       " 'distance',\n",
       " 'downtown',\n",
       " 'elbow_room',\n",
       " 'board',\n",
       " 'arrest',\n",
       " 'halt',\n",
       " 'hitch',\n",
       " 'stop',\n",
       " 'stoppage',\n",
       " 'remain',\n",
       " 'rest',\n",
       " 'stick',\n",
       " 'stick_around',\n",
       " 'stay_put',\n",
       " 'bide',\n",
       " 'abide',\n",
       " 'stay_on',\n",
       " 'continue',\n",
       " 'detain',\n",
       " 'delay',\n",
       " 'persist',\n",
       " 'last_out',\n",
       " 'ride_out',\n",
       " 'outride',\n",
       " 'quell',\n",
       " 'appease',\n",
       " 'outstanding',\n",
       " 'bang-up',\n",
       " 'bully',\n",
       " 'corking',\n",
       " 'cracking',\n",
       " 'dandy',\n",
       " 'groovy',\n",
       " 'keen',\n",
       " 'neat',\n",
       " 'nifty',\n",
       " 'not_bad',\n",
       " 'peachy',\n",
       " 'slap-up',\n",
       " 'swell',\n",
       " 'smashing',\n",
       " 'capital',\n",
       " 'majuscule',\n",
       " 'big',\n",
       " 'enceinte',\n",
       " 'expectant',\n",
       " 'gravid',\n",
       " 'heavy',\n",
       " 'with_child',\n",
       " 'Chicago',\n",
       " 'Windy_City',\n",
       " 'Michigan',\n",
       " 'Newmarket',\n",
       " 'boodle',\n",
       " 'stops',\n",
       " 'faculty',\n",
       " 'stave',\n",
       " 'nighttime',\n",
       " 'dark',\n",
       " 'Nox',\n",
       " 'Night',\n",
       " '1',\n",
       " 'I',\n",
       " 'ace',\n",
       " 'single',\n",
       " 'unity',\n",
       " 'i',\n",
       " 'ane',\n",
       " 'unitary',\n",
       " 'matchless',\n",
       " 'nonpareil',\n",
       " 'one_and_only',\n",
       " 'peerless',\n",
       " 'unmatched',\n",
       " 'unmatchable',\n",
       " 'unrivaled',\n",
       " 'unrivalled',\n",
       " 'religious_service',\n",
       " 'divine_service',\n",
       " 'military_service',\n",
       " 'armed_service',\n",
       " 'Service',\n",
       " 'Robert_William_Service',\n",
       " 'avail',\n",
       " 'help',\n",
       " 'table_service',\n",
       " 'servicing',\n",
       " 'serve',\n",
       " 'serving',\n",
       " 'service_of_process',\n",
       " 'overhaul',\n",
       " 'inspection_and_repair',\n",
       " 'bottom',\n",
       " 'seam',\n",
       " 'layer',\n",
       " 'sleep_together',\n",
       " 'roll_in_the_hay',\n",
       " 'love',\n",
       " 'make_out',\n",
       " 'make_love',\n",
       " 'sleep_with',\n",
       " 'get_laid',\n",
       " 'have_sex',\n",
       " 'know',\n",
       " 'do_it',\n",
       " 'be_intimate',\n",
       " 'have_intercourse',\n",
       " 'have_it_away',\n",
       " 'have_it_off',\n",
       " 'screw',\n",
       " 'fuck',\n",
       " 'jazz',\n",
       " 'eff',\n",
       " 'hump',\n",
       " 'lie_with',\n",
       " 'have_a_go_at_it',\n",
       " 'bang',\n",
       " 'get_it_on',\n",
       " 'bonk',\n",
       " 'go_to_bed',\n",
       " 'turn_in',\n",
       " 'crawl_in',\n",
       " 'kip_down',\n",
       " 'hit_the_hay',\n",
       " 'hit_the_sack',\n",
       " 'sack_out',\n",
       " 'go_to_sleep',\n",
       " 'retire',\n",
       " 'placement',\n",
       " 'locating',\n",
       " 'position',\n",
       " 'positioning',\n",
       " 'emplacement',\n",
       " 'localization',\n",
       " 'localisation',\n",
       " 'fix',\n",
       " 'uracil',\n",
       " 'U',\n",
       " 'uranium',\n",
       " 'atomic_number_92',\n",
       " 'acquire',\n",
       " 'become',\n",
       " 'let',\n",
       " 'have',\n",
       " 'receive',\n",
       " 'find',\n",
       " 'obtain',\n",
       " 'incur',\n",
       " 'arrive',\n",
       " 'come',\n",
       " 'bring',\n",
       " 'convey',\n",
       " 'fetch',\n",
       " 'pay_back',\n",
       " 'pay_off',\n",
       " 'induce',\n",
       " 'stimulate',\n",
       " 'cause',\n",
       " 'catch',\n",
       " 'capture',\n",
       " 'grow',\n",
       " 'develop',\n",
       " 'produce',\n",
       " 'contract',\n",
       " 'drive',\n",
       " 'aim',\n",
       " 'scram',\n",
       " 'buzz_off',\n",
       " 'fuck_off',\n",
       " 'bugger_off',\n",
       " \"get_under_one's_skin\",\n",
       " 'draw',\n",
       " 'perplex',\n",
       " 'vex',\n",
       " 'puzzle',\n",
       " 'mystify',\n",
       " 'baffle',\n",
       " 'beat',\n",
       " 'pose',\n",
       " 'bewilder',\n",
       " 'flummox',\n",
       " 'stupefy',\n",
       " 'nonplus',\n",
       " 'gravel',\n",
       " 'amaze',\n",
       " 'dumbfound',\n",
       " 'get_down',\n",
       " 'begin',\n",
       " 'start_out',\n",
       " 'start',\n",
       " 'set_about',\n",
       " 'set_out',\n",
       " 'commence',\n",
       " 'suffer',\n",
       " 'sustain',\n",
       " 'beget',\n",
       " 'engender',\n",
       " 'father',\n",
       " 'mother',\n",
       " 'sire',\n",
       " 'generate',\n",
       " 'bring_forth',\n",
       " 'clip',\n",
       " 'clock_time',\n",
       " 'fourth_dimension',\n",
       " 'meter',\n",
       " 'metre',\n",
       " 'prison_term',\n",
       " 'sentence',\n",
       " 'clock',\n",
       " 'Nice',\n",
       " 'decent',\n",
       " 'skillful',\n",
       " 'dainty',\n",
       " 'overnice',\n",
       " 'prissy',\n",
       " 'squeamish',\n",
       " 'courteous',\n",
       " 'gracious',\n",
       " 'goodness',\n",
       " 'commodity',\n",
       " 'trade_good',\n",
       " 'full',\n",
       " 'estimable',\n",
       " 'honorable',\n",
       " 'respectable',\n",
       " 'beneficial',\n",
       " 'just',\n",
       " 'upright',\n",
       " 'adept',\n",
       " 'expert',\n",
       " 'practiced',\n",
       " 'proficient',\n",
       " 'skilful',\n",
       " 'dear',\n",
       " 'near',\n",
       " 'dependable',\n",
       " 'safe',\n",
       " 'secure',\n",
       " 'ripe',\n",
       " 'effective',\n",
       " 'in_effect',\n",
       " 'in_force',\n",
       " 'serious',\n",
       " 'sound',\n",
       " 'salutary',\n",
       " 'honest',\n",
       " 'undecomposed',\n",
       " 'unspoiled',\n",
       " 'unspoilt',\n",
       " 'thoroughly',\n",
       " 'soundly',\n",
       " 'flooring',\n",
       " 'level',\n",
       " 'storey',\n",
       " 'story',\n",
       " 'base',\n",
       " 'trading_floor',\n",
       " 'shock',\n",
       " 'ball_over',\n",
       " 'blow_out_of_the_water',\n",
       " 'take_aback',\n",
       " 'deck',\n",
       " 'coldcock',\n",
       " 'dump',\n",
       " 'knock_down',\n",
       " 'bath',\n",
       " 'toilet',\n",
       " 'lavatory',\n",
       " 'lav',\n",
       " 'can',\n",
       " 'john',\n",
       " 'privy',\n",
       " 'evening',\n",
       " 'eve',\n",
       " 'eventide',\n",
       " 'flush',\n",
       " 'even_out',\n",
       " 'fifty-fifty',\n",
       " 'regular',\n",
       " 'tied',\n",
       " 'yet',\n",
       " 'twenty-four_hours',\n",
       " 'twenty-four_hour_period',\n",
       " '24-hour_interval',\n",
       " 'solar_day',\n",
       " 'mean_solar_day',\n",
       " 'daytime',\n",
       " 'daylight',\n",
       " 'sidereal_day',\n",
       " 'Day',\n",
       " 'Clarence_Day',\n",
       " 'Clarence_Shepard_Day_Jr.',\n",
       " 'besides',\n",
       " 'too',\n",
       " 'likewise',\n",
       " 'as_well',\n",
       " 'clean_and_jerk',\n",
       " 'make_clean',\n",
       " 'pick',\n",
       " 'houseclean',\n",
       " 'clean_house',\n",
       " 'cleanse',\n",
       " 'strip',\n",
       " 'scavenge',\n",
       " 'clear',\n",
       " 'light',\n",
       " 'unclouded',\n",
       " 'fresh',\n",
       " 'uncontaminating',\n",
       " 'unobjectionable',\n",
       " 'uninfected',\n",
       " 'clean-living',\n",
       " 'fair',\n",
       " 'blank',\n",
       " 'white',\n",
       " 'sporting',\n",
       " 'sporty',\n",
       " 'sportsmanlike',\n",
       " 'plumb',\n",
       " 'plum',\n",
       " 'fairly',\n",
       " 'perspective',\n",
       " 'aspect',\n",
       " 'prospect',\n",
       " 'scene',\n",
       " 'vista',\n",
       " 'panorama',\n",
       " 'survey',\n",
       " 'sight',\n",
       " 'eyeshot',\n",
       " 'opinion',\n",
       " 'sentiment',\n",
       " 'persuasion',\n",
       " 'thought',\n",
       " 'horizon',\n",
       " 'purview',\n",
       " 'see',\n",
       " 'consider',\n",
       " 'reckon',\n",
       " 'regard',\n",
       " 'look_at',\n",
       " 'watch',\n",
       " 'take_in',\n",
       " 'the_like',\n",
       " 'the_likes_of',\n",
       " 'ilk',\n",
       " 'wish',\n",
       " 'care',\n",
       " 'similar',\n",
       " 'same',\n",
       " 'alike',\n",
       " 'comparable',\n",
       " 'corresponding',\n",
       " '2',\n",
       " 'II',\n",
       " 'deuce',\n",
       " 'ii',\n",
       " 'front_end',\n",
       " 'forepart',\n",
       " 'battlefront',\n",
       " 'front_line',\n",
       " 'front_man',\n",
       " 'figurehead',\n",
       " 'nominal_head',\n",
       " 'straw_man',\n",
       " 'strawman',\n",
       " 'presence',\n",
       " 'movement',\n",
       " 'social_movement',\n",
       " 'look',\n",
       " 'face',\n",
       " 'breast',\n",
       " 'dorsum',\n",
       " 'rear',\n",
       " 'spinal_column',\n",
       " 'vertebral_column',\n",
       " 'spine',\n",
       " 'backbone',\n",
       " 'rachis',\n",
       " 'binding',\n",
       " 'book_binding',\n",
       " 'cover',\n",
       " 'backrest',\n",
       " 'endorse',\n",
       " 'indorse',\n",
       " 'plump_for',\n",
       " 'plunk_for',\n",
       " 'support',\n",
       " 'second',\n",
       " 'bet_on',\n",
       " 'gage',\n",
       " 'stake',\n",
       " 'game',\n",
       " 'punt',\n",
       " 'back_up',\n",
       " 'hind',\n",
       " 'hinder',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'rearward',\n",
       " 'rearwards',\n",
       " 'eating_house',\n",
       " 'eating_place',\n",
       " 'eatery',\n",
       " 'anteroom',\n",
       " 'antechamber',\n",
       " 'entrance_hall',\n",
       " 'hall',\n",
       " 'foyer',\n",
       " 'vestibule',\n",
       " 'pressure_group',\n",
       " 'third_house',\n",
       " 'buttonhole',\n",
       " 'favorable',\n",
       " 'well-disposed',\n",
       " 'spell',\n",
       " 'tour',\n",
       " 'turn',\n",
       " 'Adam',\n",
       " 'ecstasy',\n",
       " 'XTC',\n",
       " 'disco_biscuit',\n",
       " 'cristal',\n",
       " 'X',\n",
       " 'hug_drug',\n",
       " 'crack',\n",
       " 'fling',\n",
       " 'pass',\n",
       " 'whirl',\n",
       " 'offer',\n",
       " 'go_game',\n",
       " 'travel',\n",
       " 'move',\n",
       " 'locomote',\n",
       " 'proceed',\n",
       " 'go_away',\n",
       " 'depart',\n",
       " 'run',\n",
       " 'lead',\n",
       " 'extend',\n",
       " 'function',\n",
       " 'operate',\n",
       " 'run_low',\n",
       " 'run_short',\n",
       " 'survive',\n",
       " 'last',\n",
       " 'live',\n",
       " 'live_on',\n",
       " 'endure',\n",
       " 'hold_up',\n",
       " 'hold_out',\n",
       " 'die',\n",
       " 'decease',\n",
       " 'perish',\n",
       " 'exit',\n",
       " 'pass_away',\n",
       " 'expire',\n",
       " 'kick_the_bucket',\n",
       " \"cash_in_one's_chips\",\n",
       " 'buy_the_farm',\n",
       " 'conk',\n",
       " 'give-up_the_ghost',\n",
       " 'drop_dead',\n",
       " 'pop_off',\n",
       " 'choke',\n",
       " 'croak',\n",
       " 'snuff_it',\n",
       " 'belong',\n",
       " 'get_going',\n",
       " 'blend',\n",
       " 'blend_in',\n",
       " 'fit',\n",
       " 'rifle',\n",
       " 'plump',\n",
       " 'fail',\n",
       " 'go_bad',\n",
       " 'give_way',\n",
       " 'give_out',\n",
       " 'conk_out',\n",
       " 'break',\n",
       " 'break_down',\n",
       " 'topographic_point',\n",
       " 'spot',\n",
       " 'property',\n",
       " 'stead',\n",
       " 'lieu',\n",
       " 'shoes',\n",
       " 'home',\n",
       " 'post',\n",
       " 'berth',\n",
       " 'office',\n",
       " 'billet',\n",
       " 'situation',\n",
       " 'station',\n",
       " 'seat',\n",
       " 'plaza',\n",
       " 'piazza',\n",
       " 'space',\n",
       " 'blank_space',\n",
       " 'put',\n",
       " 'set',\n",
       " 'lay',\n",
       " 'rank',\n",
       " 'range',\n",
       " 'order',\n",
       " 'grade',\n",
       " 'locate',\n",
       " 'site',\n",
       " 'come_in',\n",
       " 'come_out',\n",
       " 'target',\n",
       " 'direct',\n",
       " 'point',\n",
       " 'identify',\n",
       " 'localize',\n",
       " 'localise',\n",
       " 'invest',\n",
       " 'commit',\n",
       " 'send',\n",
       " 'minor',\n",
       " 'modest',\n",
       " 'small-scale',\n",
       " 'pocket-size',\n",
       " 'pocket-sized',\n",
       " 'humble',\n",
       " 'low',\n",
       " 'lowly',\n",
       " 'minuscule',\n",
       " 'belittled',\n",
       " 'diminished',\n",
       " 'comfy',\n",
       " 'easy',\n",
       " 'prosperous',\n",
       " 'well-fixed',\n",
       " 'well-heeled',\n",
       " 'well-off',\n",
       " 'well-situated',\n",
       " 'well-to-do',\n",
       " 'wellspring',\n",
       " 'fountainhead',\n",
       " 'easily',\n",
       " 'considerably',\n",
       " 'substantially',\n",
       " 'intimately',\n",
       " 'advantageously',\n",
       " 'comfortably',\n",
       " 'doorway',\n",
       " 'room_access',\n",
       " 'threshold',\n",
       " 'truly',\n",
       " 'genuinely',\n",
       " 'actually',\n",
       " 'in_truth',\n",
       " 'very',\n",
       " 'real',\n",
       " 'rattling',\n",
       " 'barroom',\n",
       " 'saloon',\n",
       " 'ginmill',\n",
       " 'taproom',\n",
       " 'measure',\n",
       " 'prevention',\n",
       " 'legal_profession',\n",
       " 'legal_community',\n",
       " 'stripe',\n",
       " 'streak',\n",
       " 'cake',\n",
       " 'Browning_automatic_rifle',\n",
       " 'BAR',\n",
       " 'debar',\n",
       " 'exclude',\n",
       " 'barricade',\n",
       " 'blockade',\n",
       " 'block_off',\n",
       " 'block_up',\n",
       " 'banish',\n",
       " 'relegate',\n",
       " 'number_one',\n",
       " 'number_1',\n",
       " 'beginning',\n",
       " 'commencement',\n",
       " 'outset',\n",
       " 'get-go',\n",
       " 'kickoff',\n",
       " 'starting_time',\n",
       " 'showtime',\n",
       " 'offset',\n",
       " 'first_base',\n",
       " 'first-class_honours_degree',\n",
       " 'first_gear',\n",
       " 'low_gear',\n",
       " '1st',\n",
       " 'inaugural',\n",
       " 'initiative',\n",
       " 'initiatory',\n",
       " 'maiden',\n",
       " 'foremost',\n",
       " 'world-class',\n",
       " 'firstly',\n",
       " 'first_of_all',\n",
       " 'first_off',\n",
       " 'for_the_first_time',\n",
       " \"ne'er\",\n",
       " 'free_people',\n",
       " 'liberate',\n",
       " 'release',\n",
       " 'unloose',\n",
       " 'unloosen',\n",
       " 'loose',\n",
       " 'rid',\n",
       " 'disembarrass',\n",
       " 'dislodge',\n",
       " 'exempt',\n",
       " 'relieve',\n",
       " 'discharge',\n",
       " 'disengage',\n",
       " 'absolve',\n",
       " 'justify',\n",
       " 'relinquish',\n",
       " 'resign',\n",
       " 'give_up',\n",
       " 'unblock',\n",
       " 'unfreeze',\n",
       " 'complimentary',\n",
       " 'costless',\n",
       " 'gratis',\n",
       " 'gratuitous',\n",
       " 'detached',\n",
       " 'spare',\n",
       " 'barren',\n",
       " 'destitute',\n",
       " 'devoid',\n",
       " 'innocent',\n",
       " 'liberal',\n",
       " 'name',\n",
       " 'telephone',\n",
       " 'call_up',\n",
       " 'ring',\n",
       " 'shout',\n",
       " 'shout_out',\n",
       " 'cry',\n",
       " 'yell',\n",
       " 'scream',\n",
       " 'holler',\n",
       " 'hollo',\n",
       " 'squall',\n",
       " 'send_for',\n",
       " 'visit',\n",
       " 'call_in',\n",
       " 'address',\n",
       " 'bid',\n",
       " 'call_off',\n",
       " 'predict',\n",
       " 'foretell',\n",
       " 'prognosticate',\n",
       " 'forebode',\n",
       " 'anticipate',\n",
       " 'promise',\n",
       " 'Wolverine_State',\n",
       " 'Great_Lakes_State',\n",
       " 'MI',\n",
       " 'Lake_Michigan',\n",
       " 'country',\n",
       " 'region',\n",
       " 'sphere',\n",
       " 'domain',\n",
       " 'orbit',\n",
       " 'field',\n",
       " 'arena',\n",
       " 'expanse',\n",
       " 'surface_area',\n",
       " 'book',\n",
       " 'reserve',\n",
       " 'hold',\n",
       " 'engaged',\n",
       " 'set-aside',\n",
       " 'monetary_value',\n",
       " 'cost',\n",
       " 'terms',\n",
       " 'damage',\n",
       " 'toll',\n",
       " 'Price',\n",
       " 'Leontyne_Price',\n",
       " 'Mary_Leontyne_Price',\n",
       " 'state',\n",
       " 'tell',\n",
       " 'narrate',\n",
       " 'recount',\n",
       " 'recite',\n",
       " 'enjoin',\n",
       " 'assure',\n",
       " 'evidence',\n",
       " 'distinguish',\n",
       " 'separate',\n",
       " 'differentiate',\n",
       " 'secern',\n",
       " 'secernate',\n",
       " 'severalize',\n",
       " 'severalise',\n",
       " 'tell_apart',\n",
       " 'following',\n",
       " 'adjacent',\n",
       " 'side_by_side',\n",
       " 'future',\n",
       " 'succeeding',\n",
       " 'bettor',\n",
       " 'wagerer',\n",
       " 'punter',\n",
       " 'improve',\n",
       " 'amend',\n",
       " 'ameliorate',\n",
       " 'meliorate',\n",
       " 'min',\n",
       " 'moment',\n",
       " 'mo',\n",
       " 'bit',\n",
       " 'instant',\n",
       " 'arcminute',\n",
       " 'minute_of_arc',\n",
       " 'infinitesimal',\n",
       " 'narrow',\n",
       " 'urge',\n",
       " 'advocate',\n",
       " 'commend',\n",
       " 'invitee',\n",
       " 'Guest',\n",
       " 'Edgar_Guest',\n",
       " 'Edgar_Albert_Guest',\n",
       " 'node',\n",
       " 'client',\n",
       " 'morn',\n",
       " 'morning_time',\n",
       " 'forenoon',\n",
       " 'good_morning',\n",
       " 'dawn',\n",
       " 'dawning',\n",
       " 'aurora',\n",
       " 'first_light',\n",
       " 'daybreak',\n",
       " 'break_of_day',\n",
       " 'break_of_the_day',\n",
       " 'dayspring',\n",
       " 'sunrise',\n",
       " 'sunup',\n",
       " 'cockcrow',\n",
       " 'go_through',\n",
       " 'feel',\n",
       " 'do',\n",
       " 'create',\n",
       " 'gain',\n",
       " 'earn',\n",
       " 'realize',\n",
       " 'realise',\n",
       " 'pull_in',\n",
       " 'bring_in',\n",
       " 'form',\n",
       " 'constitute',\n",
       " 'reach',\n",
       " 'get_to',\n",
       " 'progress_to',\n",
       " 'construct',\n",
       " 'build',\n",
       " 'nominate',\n",
       " 'attain',\n",
       " 'hit',\n",
       " 'arrive_at',\n",
       " 'lay_down',\n",
       " 'establish',\n",
       " 'throw',\n",
       " 'give',\n",
       " 'make_up',\n",
       " 'stool',\n",
       " 'defecate',\n",
       " 'shit',\n",
       " 'take_a_shit',\n",
       " 'take_a_crap',\n",
       " 'ca-ca',\n",
       " 'crap',\n",
       " 'cook',\n",
       " 'ready',\n",
       " 'prepare',\n",
       " 'seduce',\n",
       " 'score',\n",
       " 'pretend',\n",
       " 'make_believe',\n",
       " 'urinate',\n",
       " 'piddle',\n",
       " 'puddle',\n",
       " 'micturate',\n",
       " 'piss',\n",
       " 'pee',\n",
       " 'pee-pee',\n",
       " 'make_water',\n",
       " 'relieve_oneself',\n",
       " 'take_a_leak',\n",
       " 'spend_a_penny',\n",
       " 'wee',\n",
       " 'wee-wee',\n",
       " 'pass_water',\n",
       " 'rooms',\n",
       " 'cortege',\n",
       " 'retinue',\n",
       " 'entourage',\n",
       " 'concern',\n",
       " 'business_concern',\n",
       " 'business_organization',\n",
       " 'business_organisation',\n",
       " 'commercial_enterprise',\n",
       " 'business_enterprise',\n",
       " 'occupation',\n",
       " 'job',\n",
       " 'line_of_work',\n",
       " 'line',\n",
       " 'business_sector',\n",
       " 'clientele',\n",
       " 'patronage',\n",
       " 'stage_business',\n",
       " 'byplay',\n",
       " 'lift',\n",
       " 'a_lot',\n",
       " 'lots',\n",
       " 'a_good_deal',\n",
       " 'a_great_deal',\n",
       " 'very_much',\n",
       " 'practically',\n",
       " 'often',\n",
       " 'bank_check',\n",
       " 'cheque',\n",
       " 'assay',\n",
       " 'chit',\n",
       " 'tab',\n",
       " 'confirmation',\n",
       " 'verification',\n",
       " 'substantiation',\n",
       " 'checkout',\n",
       " 'check-out_procedure',\n",
       " 'check_mark',\n",
       " 'tick',\n",
       " 'hindrance',\n",
       " 'hinderance',\n",
       " 'deterrent',\n",
       " 'impediment',\n",
       " 'balk',\n",
       " 'baulk',\n",
       " 'handicap',\n",
       " 'chip',\n",
       " 'bridle',\n",
       " 'curb',\n",
       " 'check_up_on',\n",
       " 'look_into',\n",
       " 'check_out',\n",
       " ...]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notfake_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFakenessValue(review):\n",
    "    senti = 0\n",
    "    words = [word.lower() for word in tokenizer.tokenize(review)]\n",
    "    for word in words:\n",
    "        if lemmatizer.lemmatize(word) in notfake_vocab:\n",
    "            senti += 1\n",
    "        if lemmatizer.lemmatize(word) in fake_vocab:\n",
    "            senti -= 1\n",
    "    return senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['fakescore'] = test_df['text'].apply(getFakenessValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['vader_sent_score'] = test_df['text'].apply(vaderSentimentValue)\n",
    "test_df['sentiment_score'] = test_df['text'].apply(getSentimentValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deceptive</th>\n",
       "      <th>hotel</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>fakescore</th>\n",
       "      <th>vader_sent_score</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>true_label</th>\n",
       "      <th>index</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>hyatt</td>\n",
       "      <td>negative</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>Hyatt Regency Chicago seemed like a nice place...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.8073</td>\n",
       "      <td>-3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>fairmont</td>\n",
       "      <td>negative</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>My wife and I live in the Western Suburbs of C...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9416</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>truthful</td>\n",
       "      <td>allegro</td>\n",
       "      <td>negative</td>\n",
       "      <td>Web</td>\n",
       "      <td>This hotel must have originally been an ordina...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7978</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>truthful</td>\n",
       "      <td>intercontinental</td>\n",
       "      <td>positive</td>\n",
       "      <td>TripAdvisor</td>\n",
       "      <td>Stayed at the InterContinental for an entire w...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8791</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>truthful</td>\n",
       "      <td>james</td>\n",
       "      <td>negative</td>\n",
       "      <td>Web</td>\n",
       "      <td>Very disapointed in the service and quality of...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>truthful</td>\n",
       "      <td>hyatt</td>\n",
       "      <td>positive</td>\n",
       "      <td>TripAdvisor</td>\n",
       "      <td>I booked a room at the Hyatt through Priceline...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>475</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>truthful</td>\n",
       "      <td>homewood</td>\n",
       "      <td>negative</td>\n",
       "      <td>Web</td>\n",
       "      <td>This hotel is trading on the Hilton name and a...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8360</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "      <td>476</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>knickerbocker</td>\n",
       "      <td>positive</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>This hotel was worth every cent. You have not ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.8243</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>477</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>truthful</td>\n",
       "      <td>amalfi</td>\n",
       "      <td>positive</td>\n",
       "      <td>TripAdvisor</td>\n",
       "      <td>This hotel was wonderful! I am a college stude...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.9752</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>478</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>deceptive</td>\n",
       "      <td>ambassador</td>\n",
       "      <td>positive</td>\n",
       "      <td>MTurk</td>\n",
       "      <td>This was the best choice my husband and I coul...</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>479</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      deceptive             hotel  polarity       source  \\\n",
       "1220  deceptive             hyatt  negative        MTurk   \n",
       "1200  deceptive          fairmont  negative        MTurk   \n",
       "1139   truthful           allegro  negative          Web   \n",
       "353    truthful  intercontinental  positive  TripAdvisor   \n",
       "1103   truthful             james  negative          Web   \n",
       "...         ...               ...       ...          ...   \n",
       "29     truthful             hyatt  positive  TripAdvisor   \n",
       "949    truthful          homewood  negative          Web   \n",
       "546   deceptive     knickerbocker  positive        MTurk   \n",
       "398    truthful            amalfi  positive  TripAdvisor   \n",
       "611   deceptive        ambassador  positive        MTurk   \n",
       "\n",
       "                                                   text  fakescore  \\\n",
       "1220  Hyatt Regency Chicago seemed like a nice place...         -1   \n",
       "1200  My wife and I live in the Western Suburbs of C...         -1   \n",
       "1139  This hotel must have originally been an ordina...          0   \n",
       "353   Stayed at the InterContinental for an entire w...          2   \n",
       "1103  Very disapointed in the service and quality of...          0   \n",
       "...                                                 ...        ...   \n",
       "29    I booked a room at the Hyatt through Priceline...         -1   \n",
       "949   This hotel is trading on the Hilton name and a...          0   \n",
       "546   This hotel was worth every cent. You have not ...         -1   \n",
       "398   This hotel was wonderful! I am a college stude...          2   \n",
       "611   This was the best choice my husband and I coul...         -2   \n",
       "\n",
       "      vader_sent_score  sentiment_score  true_label  index  predicted_label  \n",
       "1220           -0.8073               -3           1      0               -1  \n",
       "1200            0.9416                7           1      1               -1  \n",
       "1139            0.7978                1          -1      2               -1  \n",
       "353             0.8791                9          -1      3               -1  \n",
       "1103           -0.2960               -1          -1      4               -1  \n",
       "...                ...              ...         ...    ...              ...  \n",
       "29              0.9687                5          -1    475               -1  \n",
       "949            -0.8360               -2          -1    476               -1  \n",
       "546             0.8243                3           1    477               -1  \n",
       "398             0.9752                4          -1    478               -1  \n",
       "611             0.9933               13           1    479               -1  \n",
       "\n",
       "[480 rows x 11 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = 54\n",
    "P2 = 1393 \n",
    "#for these 2 values the below two classifiers gave 94 and 73% accuracy respectively\n",
    "#1203,76   1161,150 (1163, 1399) (662, 1476) (1542, 194) (78, 28) (699, 44) (1384, 192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    p1 = random.randrange(len(test_df))\n",
    "    p2 = random.randrange(len(test_df))\n",
    "    if(p1 != p2):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 229)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p1 = P1\n",
    "# p2=P2\n",
    "p1,p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "point12 = list(test_df['fakescore'])[p1]\n",
    "\n",
    "point22 = list(test_df['fakescore'])[p2]\n",
    "\n",
    "def Distance(x1,y1,x2,y2):\n",
    "    return (x1-x2)**2 + (y1-y2)**2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster12 = {p1:point12}\n",
    "\n",
    "cluster22 = {p2:point22}\n",
    "\n",
    "old_centroid12 = new_centroid12 = point12\n",
    "\n",
    "old_centroid22 = new_centroid22 = point22\n",
    "\n",
    "\n",
    "while(True):\n",
    "   # old_centroid11 = new_centroid11\n",
    "    old_centroid12 = new_centroid12\n",
    "    #old_centroid21 = new_centroid21\n",
    "    old_centroid22 = new_centroid22\n",
    "    for i in test_df.index:\n",
    "       # point1 = test_df['vader_sent_score'][i]\n",
    "        point2 = test_df['fakescore'][i]\n",
    "        \n",
    "        if(Distance(0,old_centroid12,0,point2) > Distance(0,old_centroid22,0,point2)):\n",
    "            if(i not in cluster22):\n",
    "               # cluster21[i] = point1\n",
    "                cluster22[i] = point2\n",
    "        else:\n",
    "            if(i not in cluster12):\n",
    "               # cluster11[i] = point1\n",
    "                cluster12[i] = point2\n",
    "        \n",
    "        #new_centroid11 = sum(cluster11.values())/len(cluster11)\n",
    "        new_centroid12 = sum(cluster12.values())/len(cluster12)\n",
    "       # new_centroid21 = sum(cluster21.values())/len(cluster21)\n",
    "        new_centroid22 = sum(cluster22.values())/len(cluster22)\n",
    "    if( old_centroid12 == new_centroid12 and old_centroid22 == new_centroid22):\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Distance(0,old_centroid12,0,0) < Distance(0,old_centroid22,0,0)):\n",
    "    label1 = 1 #fake\n",
    "    label2 = -1 #real\n",
    "else:\n",
    "    label1 = -1\n",
    "    label2 = 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGivenLabel(value):\n",
    "    if( value == \"truthful\"):\n",
    "        return -1 #notfake\n",
    "    else:\n",
    "        return 1 #fake\n",
    "\n",
    "test_df['true_label'] = test_df['deceptive'].apply(getGivenLabel)\n",
    "\n",
    "test_df['index'] = list(range(len(test_df)))\n",
    "\n",
    "def getPredictedLabel(value):  \n",
    "    if(value in cluster11):\n",
    "        return label1\n",
    "    else:\n",
    "        return label2\n",
    "    \n",
    "test_df['predicted_label'] = test_df['index'].apply(getPredictedLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48541666666666666"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in test_df.index:\n",
    "    if(test_df['true_label'][i] == test_df['predicted_label'][i]):\n",
    "        count += 1\n",
    "            \n",
    "count/len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "truePositive = trueNegative = falsePositive = falseNegative = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_df.index:\n",
    "    if(test_df['true_label'][i] == test_df['predicted_label'][i] == 1):\n",
    "        truePositive += 1\n",
    "    elif test_df['true_label'][i] == test_df['predicted_label'][i] == -1:\n",
    "        trueNegative += 1\n",
    "    elif test_df['true_label'][i] == -1 and test_df['predicted_label'][i] == 1:\n",
    "        falsePositive += 1\n",
    "    elif test_df['true_label'][i] == 1 and test_df['predicted_label'][i] == -1 :\n",
    "        falseNegative += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.48541666666666666, 0.5027027027027027, 0.7469879518072289)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "precision = truePositive/(truePositive + falsePositive)\n",
    "recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "accuracy,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7091412742382271, 0.696969696969697, 0.7540983606557377)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "for i in test_df.index:\n",
    "    if(test_df['true_label'][i] == 1 and test_df['fakescore'][i] < 0):\n",
    "        truePositive += 1\n",
    "    elif test_df['true_label'][i] == -1 and test_df['fakescore'][i] > 0:\n",
    "        trueNegative += 1\n",
    "    elif test_df['true_label'][i] == -1 and test_df['fakescore'][i] < 0:\n",
    "        falsePositive += 1\n",
    "    elif test_df['true_label'][i] == 1 and test_df['fakescore'][i] > 0 :\n",
    "        falseNegative += 1\n",
    "        \n",
    "accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "precision = truePositive/(truePositive + falsePositive)\n",
    "recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "accuracy,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    p1 = random.randrange(len(test_df))\n",
    "    p2 = random.randrange(len(test_df))\n",
    "    if(p1 != p2):\n",
    "        break\n",
    "point1 = list(test_df['fakescore'])[p1]\n",
    "\n",
    "point2 = list(test_df['fakescore'])[p2]\n",
    "\n",
    "def Distance(x1,x2):\n",
    "    return (x1-x2)**2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1 = {p1:point1}\n",
    "\n",
    "cluster2 = {p2:point2}\n",
    "\n",
    "old_centroid1 = new_centroid1 = point1\n",
    "\n",
    "old_centroid2 = new_centroid2 = point2\n",
    "\n",
    "\n",
    "\n",
    "while(True):\n",
    "    old_centroid1 = new_centroid1\n",
    "\n",
    "    old_centroid2 = new_centroid2\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "       \n",
    "        point = list(test_df['fakescore'])[i]\n",
    "        \n",
    "        if(Distance(old_centroid1,point) > Distance(old_centroid2,point)):\n",
    "            if(i not in cluster2):\n",
    "                cluster2[i] = point\n",
    "        else:\n",
    "            if(i not in cluster11):\n",
    "                cluster1[i] = point\n",
    "               \n",
    "        \n",
    "        new_centroid1 = sum(cluster1.values())/len(cluster1)\n",
    "        \n",
    "        new_centroid2 = sum(cluster2.values())/len(cluster2)\n",
    "    if(old_centroid1 == new_centroid1 and old_centroid2 == new_centroid2):\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Distance(old_centroid1,0) < Distance(old_centroid2,0)):\n",
    "    label1 = -1\n",
    "    label2 = 1\n",
    "else:\n",
    "    label1 = 1\n",
    "    label2 = -1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getPredictedLabel(value):  \n",
    "    if(value in cluster1):\n",
    "        return label1\n",
    "    else:\n",
    "        return label2\n",
    "    \n",
    "test_df['predicted_label'] = test_df['index'].apply(getPredictedLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6354166666666666, 0.6434108527131783, 0.6666666666666666)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "for i in test_df.index:\n",
    "    if(test_df['true_label'][i] == test_df['predicted_label'][i] == 1):\n",
    "        truePositive += 1\n",
    "    elif test_df['true_label'][i] == test_df['predicted_label'][i] == -1:\n",
    "        trueNegative += 1\n",
    "    elif test_df['true_label'][i] == -1 and test_df['predicted_label'][i] == 1:\n",
    "        falsePositive += 1\n",
    "    elif test_df['true_label'][i] == 1 and test_df['predicted_label'][i] == -1 :\n",
    "        falseNegative += 1\n",
    "\n",
    "accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "precision = truePositive/(truePositive + falsePositive)\n",
    "recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "accuracy,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for p1=274 and p2=453 the accuracy given by this is 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_vocab = {}\n",
    "notfake_vocab = {}\n",
    "for i in range(len(train_df)):\n",
    "    review = list(train_df['text'])[i]\n",
    "    words = word_tokenize(review)\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words and len(word) != 1:\n",
    "            if list(train_df['deceptive'])[i] == \"truthful\":\n",
    "                if lemmatizer.lemmatize(word.lower()) not in notfake_vocab:\n",
    "                    notfake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                else:\n",
    "                    notfake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "            elif lemmatizer.lemmatize(word.lower()) not in fake_vocab:\n",
    "                fake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "            else:\n",
    "                fake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "\n",
    "\n",
    "fake_vocab = sorted(fake_vocab.items(), key = \n",
    "         lambda kv:(-kv[1], kv[0]))   \n",
    "notfake_vocab =sorted(notfake_vocab.items(), key = \n",
    "             lambda kv:(-kv[1], kv[0]))  \n",
    "finalFakeVocab = []\n",
    "finalNotFakeVocab = []\n",
    "for i in range(200):\n",
    "    finalFakeVocab.append(fake_vocab[i][0])\n",
    "    finalNotFakeVocab.append(notfake_vocab[i][0])\n",
    "fake_vocab = finalFakeVocab\n",
    "notfake_vocab = finalNotFakeVocab\n",
    "\n",
    "old_len_fakevocab = len(fake_vocab)\n",
    "old_len_notfakevocab = len(notfake_vocab)\n",
    "new_len_fakevocab = 0;\n",
    "new_len_notfakevocab = 0;\n",
    "\n",
    "while(True):\n",
    "    old_len_fakevocab = len(fake_vocab)\n",
    "    old_len_notfakevocab = len(notfake_vocab)\n",
    "    for word in notfake_vocab:\n",
    "        for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                            for lemma in synset.lemmas():\n",
    "                                if lemma.name() not in notfake_vocab:\n",
    "                                    notfake_vocab.append(lemma.name())\n",
    "\n",
    "\n",
    "\n",
    "    for word in fake_vocab:\n",
    "        for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                            for lemma in synset.lemmas():\n",
    "                                if lemma.name() not in fake_vocab:\n",
    "                                    fake_vocab.append(lemma.name())\n",
    "\n",
    "    new_len_fakevocab = len(fake_vocab);\n",
    "    new_len_notfakevocab = len(notfake_vocab);\n",
    "\n",
    "    if(old_len_fakevocab == new_len_fakevocab and old_len_notfakevocab == new_len_notfakevocab):\n",
    "        break\n",
    "        \n",
    "test_df['fakescore'] = test_df['text'].apply(getFakenessValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    p1 = random.randrange(len(test_df))\n",
    "    p2 = random.randrange(len(test_df))\n",
    "    if(p1 != p2):\n",
    "        break\n",
    "point1 = list(test_df['fakescore'])[p1]\n",
    "\n",
    "point2 = list(test_df['fakescore'])[p2]\n",
    "\n",
    "def Distance(x1,x2):\n",
    "    return (x1-x2)**2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1 = {p1:point1}\n",
    "\n",
    "cluster2 = {p2:point2}\n",
    "\n",
    "old_centroid1 = new_centroid1 = point1\n",
    "\n",
    "old_centroid2 = new_centroid2 = point2\n",
    "\n",
    "\n",
    "\n",
    "while(True):\n",
    "    old_centroid1 = new_centroid1\n",
    "\n",
    "    old_centroid2 = new_centroid2\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "       \n",
    "        point = list(test_df['fakescore'])[i]\n",
    "        \n",
    "        if(Distance(old_centroid1,point) > Distance(old_centroid2,point)):\n",
    "            if(i not in cluster2):\n",
    "                cluster2[i] = point\n",
    "        else:\n",
    "            if(i not in cluster11):\n",
    "                cluster1[i] = point\n",
    "               \n",
    "        \n",
    "        new_centroid1 = sum(cluster1.values())/len(cluster1)\n",
    "        \n",
    "        new_centroid2 = sum(cluster2.values())/len(cluster2)\n",
    "    if(old_centroid1 == new_centroid1 and old_centroid2 == new_centroid2):\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(Distance(old_centroid1,0) < Distance(old_centroid2,0)):\n",
    "    label1 = -1\n",
    "    label2 = 1\n",
    "else:\n",
    "    label1 = 1\n",
    "    label2 = -1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGivenLabel(value):\n",
    "    if( value == \"truthful\"):\n",
    "        return -1 #notfake\n",
    "    else:\n",
    "        return 1 #fake\n",
    "\n",
    "test_df['true_label'] = test_df['deceptive'].apply(getGivenLabel)\n",
    "\n",
    "test_df['index'] = list(range(len(test_df)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getPredictedLabel(value):  \n",
    "    if(value in cluster1):\n",
    "        return label1\n",
    "    else:\n",
    "        return label2\n",
    "    \n",
    "test_df['predicted_label'] = test_df['index'].apply(getPredictedLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6479166666666667, 0.6272401433691757, 0.7291666666666666)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "for i in test_df.index:\n",
    "    if(test_df['true_label'][i] == test_df['predicted_label'][i] == 1):\n",
    "        truePositive += 1\n",
    "    elif test_df['true_label'][i] == test_df['predicted_label'][i] == -1:\n",
    "        trueNegative += 1\n",
    "    elif test_df['true_label'][i] == -1 and test_df['predicted_label'][i] == 1:\n",
    "        falsePositive += 1\n",
    "    elif test_df['true_label'][i] == 1 and test_df['predicted_label'][i] == -1 :\n",
    "        falseNegative += 1\n",
    "\n",
    "accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "precision = truePositive/(truePositive + falsePositive)\n",
    "recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "accuracy,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_vocab = {}\n",
    "notfake_vocab = {}\n",
    "for i in range(len(train_df)):\n",
    "    review = list(train_df['text'])[i]\n",
    "    words = word_tokenize(review)\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words and len(word) != 1:\n",
    "            if list(train_df['deceptive'])[i] == \"truthful\":\n",
    "                if lemmatizer.lemmatize(word.lower()) not in notfake_vocab:\n",
    "                    notfake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                else:\n",
    "                    notfake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "            elif lemmatizer.lemmatize(word.lower()) not in fake_vocab:\n",
    "                fake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "            else:\n",
    "                fake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "\n",
    "\n",
    "fake_vocab = sorted(fake_vocab.items(), key = \n",
    "         lambda kv:(-kv[1], kv[0]))   \n",
    "notfake_vocab =sorted(notfake_vocab.items(), key = \n",
    "             lambda kv:(-kv[1], kv[0]))  \n",
    "finalFakeVocab = []\n",
    "finalNotFakeVocab = []\n",
    "for i in range(500):\n",
    "    finalFakeVocab.append(fake_vocab[i][0])\n",
    "    finalNotFakeVocab.append(notfake_vocab[i][0])\n",
    "fake_vocab = finalFakeVocab\n",
    "notfake_vocab = finalNotFakeVocab\n",
    "\n",
    "old_len_fakevocab = len(fake_vocab)\n",
    "old_len_notfakevocab = len(notfake_vocab)\n",
    "new_len_fakevocab = 0;\n",
    "new_len_notfakevocab = 0;\n",
    "\n",
    "while(True):\n",
    "    old_len_fakevocab = len(fake_vocab)\n",
    "    old_len_notfakevocab = len(notfake_vocab)\n",
    "    for word in notfake_vocab:\n",
    "        for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                            for lemma in synset.lemmas():\n",
    "                                if lemma.name() not in notfake_vocab:\n",
    "                                    notfake_vocab.append(lemma.name())\n",
    "\n",
    "\n",
    "\n",
    "    for word in fake_vocab:\n",
    "        for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                            for lemma in synset.lemmas():\n",
    "                                if lemma.name() not in fake_vocab:\n",
    "                                    fake_vocab.append(lemma.name())\n",
    "\n",
    "    new_len_fakevocab = len(fake_vocab);\n",
    "    new_len_notfakevocab = len(notfake_vocab);\n",
    "\n",
    "    if(old_len_fakevocab == new_len_fakevocab and old_len_notfakevocab == new_len_notfakevocab):\n",
    "        break\n",
    "        \n",
    "test_df['fakescore'] = test_df['text'].apply(getFakenessValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    p1 = random.randrange(len(test_df))\n",
    "    p2 = random.randrange(len(test_df))\n",
    "    if(p1 != p2):\n",
    "        break\n",
    "point1 = list(test_df['fakescore'])[p1]\n",
    "\n",
    "point2 = list(test_df['fakescore'])[p2]\n",
    "\n",
    "def Distance(x1,x2):\n",
    "    return (x1-x2)**2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1 = {p1:point1}\n",
    "\n",
    "cluster2 = {p2:point2}\n",
    "\n",
    "old_centroid1 = new_centroid1 = point1\n",
    "\n",
    "old_centroid2 = new_centroid2 = point2\n",
    "\n",
    "\n",
    "\n",
    "while(True):\n",
    "    old_centroid1 = new_centroid1\n",
    "\n",
    "    old_centroid2 = new_centroid2\n",
    "    \n",
    "    for i in range(len(test_df)):\n",
    "       \n",
    "        point = list(test_df['fakescore'])[i]\n",
    "        \n",
    "        if(Distance(old_centroid1,point) > Distance(old_centroid2,point)):\n",
    "            if(i not in cluster2):\n",
    "                cluster2[i] = point\n",
    "        else:\n",
    "            if(i not in cluster11):\n",
    "                cluster1[i] = point\n",
    "               \n",
    "        \n",
    "        new_centroid1 = sum(cluster1.values())/len(cluster1)\n",
    "        \n",
    "        new_centroid2 = sum(cluster2.values())/len(cluster2)\n",
    "    if(old_centroid1 == new_centroid1 and old_centroid2 == new_centroid2):\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGivenLabel(value):\n",
    "    if( value == \"truthful\"):\n",
    "        return -1 #notfake\n",
    "    else:\n",
    "        return 1 #fake\n",
    "\n",
    "test_df['true_label'] = test_df['deceptive'].apply(getGivenLabel)\n",
    "\n",
    "test_df['index'] = list(range(len(test_df)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getPredictedLabel(value):  \n",
    "    if(value in cluster1):\n",
    "        return label1\n",
    "    else:\n",
    "        return label2\n",
    "    \n",
    "test_df['predicted_label'] = test_df['index'].apply(getPredictedLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5958333333333333, 0.6236559139784946, 0.48333333333333334)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "for i in test_df.index:\n",
    "    if(test_df['true_label'][i] == test_df['predicted_label'][i] == 1):\n",
    "        truePositive += 1\n",
    "    elif test_df['true_label'][i] == test_df['predicted_label'][i] == -1:\n",
    "        trueNegative += 1\n",
    "    elif test_df['true_label'][i] == -1 and test_df['predicted_label'][i] == 1:\n",
    "        falsePositive += 1\n",
    "    elif test_df['true_label'][i] == 1 and test_df['predicted_label'][i] == -1 :\n",
    "        falseNegative += 1\n",
    "\n",
    "accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "precision = truePositive/(truePositive + falsePositive)\n",
    "recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "accuracy,precision,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
