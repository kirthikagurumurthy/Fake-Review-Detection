{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import treebank\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "import math\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('deceptive-opinion.csv')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df,test_size):\n",
    "    if isinstance(test_size,float):\n",
    "        test_size = round(test_size * len(df))\n",
    "        \n",
    "    if(test_size > len(df)):\n",
    "        return 0, df\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices,k = test_size)\n",
    "\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    \n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = [0]*5\n",
    "test_df = [0]*5\n",
    "for i in range(5):\n",
    "    train_df[i],test_df[i] = train_test_split(dataset,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = treebank.TreebankWordTokenizer()\n",
    "def getFakenessValue(review):\n",
    "    senti = 0\n",
    "    words = [word.lower() for word in tokenizer.tokenize(review)]\n",
    "    for word in words:\n",
    "        if lemmatizer.lemmatize(word) in notfake_vocab:\n",
    "            senti += 1\n",
    "        if lemmatizer.lemmatize(word) in fake_vocab:\n",
    "            senti -= 1\n",
    "    return senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "\n",
    "for j in range(5):\n",
    "\n",
    "    fake_vocab = {}\n",
    "    notfake_vocab = {}\n",
    "    for i in range(len(train_df[j])):\n",
    "        review = list(train_df[j]['text'])[i]\n",
    "        words = word_tokenize(review)\n",
    "        for word in words:\n",
    "            if word.lower() not in stop_words and len(word) != 1:\n",
    "                if list(train_df[j]['deceptive'])[i] == \"truthful\":\n",
    "                    if lemmatizer.lemmatize(word.lower()) not in notfake_vocab:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                    else:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                elif lemmatizer.lemmatize(word.lower()) not in fake_vocab:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "                else:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "    \n",
    "    \n",
    "    fake_vocab = sorted(fake_vocab.items(), key = \n",
    "             lambda kv:(-kv[1], kv[0]))   \n",
    "    notfake_vocab =sorted(notfake_vocab.items(), key = \n",
    "                 lambda kv:(-kv[1], kv[0]))  \n",
    "    finalFakeVocab = []\n",
    "    finalNotFakeVocab = []\n",
    "    for i in range(2000):\n",
    "        finalFakeVocab.append(fake_vocab[i][0])\n",
    "        finalNotFakeVocab.append(notfake_vocab[i][0])\n",
    "    fake_vocab = finalFakeVocab\n",
    "    notfake_vocab = finalNotFakeVocab\n",
    "    \n",
    "    old_len_fakevocab = len(fake_vocab)\n",
    "    old_len_notfakevocab = len(notfake_vocab)\n",
    "    new_len_fakevocab = 0;\n",
    "    new_len_notfakevocab = 0;\n",
    "    \n",
    "    while(True):\n",
    "        old_len_fakevocab = len(fake_vocab)\n",
    "        old_len_notfakevocab = len(notfake_vocab)\n",
    "        for word in notfake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in notfake_vocab:\n",
    "                                        notfake_vocab.append(lemma.name())\n",
    "\n",
    "\n",
    "\n",
    "        for word in fake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in fake_vocab:\n",
    "                                        fake_vocab.append(lemma.name())\n",
    "        \n",
    "        new_len_fakevocab = len(fake_vocab);\n",
    "        new_len_notfakevocab = len(notfake_vocab);\n",
    "        \n",
    "        if(old_len_fakevocab == new_len_fakevocab and old_len_notfakevocab == new_len_notfakevocab):\n",
    "            break\n",
    "\n",
    "    \n",
    "    test_df[j]['fakescore'] = test_df[j]['text'].apply(getFakenessValue)\n",
    "    \n",
    "    truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "    for i in range(len(test_df[j])):\n",
    "        if list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            truePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] > 0:\n",
    "            trueNegative += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            falsePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] > 0 :\n",
    "            falseNegative += 1\n",
    "\n",
    "    accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "    precision = truePositive/(truePositive + falsePositive)\n",
    "    recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30339, 30186)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(notfake_vocab),len(fake_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7053990644386113, 0.700336759223777, 0.63725296777434)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(accuracies)/len(accuracies),sum(precisions)/len(precisions),sum(recalls)/len(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "\n",
    "for j in range(5):\n",
    "\n",
    "    fake_vocab = {}\n",
    "    notfake_vocab = {}\n",
    "    for i in range(len(train_df[j])):\n",
    "        review = list(train_df[j]['text'])[i]\n",
    "        words = word_tokenize(review)\n",
    "        for word in words:\n",
    "            if word.lower() not in stop_words and len(word) != 1:\n",
    "                if list(train_df[j]['deceptive'])[i] == \"truthful\":\n",
    "                    if lemmatizer.lemmatize(word.lower()) not in notfake_vocab:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                    else:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                elif lemmatizer.lemmatize(word.lower()) not in fake_vocab:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "                else:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "    \n",
    "    \n",
    "    fake_vocab = sorted(fake_vocab.items(), key = \n",
    "             lambda kv:(-kv[1], kv[0]))   \n",
    "    notfake_vocab =sorted(notfake_vocab.items(), key = \n",
    "                 lambda kv:(-kv[1], kv[0]))  \n",
    "    finalFakeVocab = []\n",
    "    finalNotFakeVocab = []\n",
    "    for i in range(1000):\n",
    "        finalFakeVocab.append(fake_vocab[i][0])\n",
    "        finalNotFakeVocab.append(notfake_vocab[i][0])\n",
    "    fake_vocab = finalFakeVocab\n",
    "    notfake_vocab = finalNotFakeVocab\n",
    "    \n",
    "    old_len_fakevocab = len(fake_vocab)\n",
    "    old_len_notfakevocab = len(notfake_vocab)\n",
    "    new_len_fakevocab = 0;\n",
    "    new_len_notfakevocab = 0;\n",
    "    \n",
    "    while(True):\n",
    "        old_len_fakevocab = len(fake_vocab)\n",
    "        old_len_notfakevocab = len(notfake_vocab)\n",
    "        for word in notfake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in notfake_vocab:\n",
    "                                        notfake_vocab.append(lemma.name())\n",
    "\n",
    "\n",
    "\n",
    "        for word in fake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in fake_vocab:\n",
    "                                        fake_vocab.append(lemma.name())\n",
    "        \n",
    "        new_len_fakevocab = len(fake_vocab);\n",
    "        new_len_notfakevocab = len(notfake_vocab);\n",
    "        \n",
    "        if(old_len_fakevocab == new_len_fakevocab and old_len_notfakevocab == new_len_notfakevocab):\n",
    "            break\n",
    "\n",
    "    \n",
    "    test_df[j]['fakescore'] = test_df[j]['text'].apply(getFakenessValue)\n",
    "    \n",
    "    truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "    for i in range(len(test_df[j])):\n",
    "        if list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            truePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] > 0:\n",
    "            trueNegative += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            falsePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] > 0 :\n",
    "            falseNegative += 1\n",
    "\n",
    "    accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "    precision = truePositive/(truePositive + falsePositive)\n",
    "    recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6968231621864379, 0.6679930937935936, 0.7516682922406642)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(accuracies)/len(accuracies),sum(precisions)/len(precisions),sum(recalls)/len(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "\n",
    "for j in range(5):\n",
    "\n",
    "    fake_vocab = {}\n",
    "    notfake_vocab = {}\n",
    "    for i in range(len(train_df[j])):\n",
    "        review = list(train_df[j]['text'])[i]\n",
    "        words = word_tokenize(review)\n",
    "        for word in words:\n",
    "            if word.lower() not in stop_words and len(word) != 1:\n",
    "                if list(train_df[j]['deceptive'])[i] == \"truthful\":\n",
    "                    if lemmatizer.lemmatize(word.lower()) not in notfake_vocab:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                    else:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                elif lemmatizer.lemmatize(word.lower()) not in fake_vocab:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "                else:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "    \n",
    "    \n",
    "    fake_vocab = sorted(fake_vocab.items(), key = \n",
    "             lambda kv:(-kv[1], kv[0]))   \n",
    "    notfake_vocab =sorted(notfake_vocab.items(), key = \n",
    "                 lambda kv:(-kv[1], kv[0]))  \n",
    "    finalFakeVocab = []\n",
    "    finalNotFakeVocab = []\n",
    "    for i in range(800):\n",
    "        finalFakeVocab.append(fake_vocab[i][0])\n",
    "        finalNotFakeVocab.append(notfake_vocab[i][0])\n",
    "    fake_vocab = finalFakeVocab\n",
    "    notfake_vocab = finalNotFakeVocab\n",
    "    \n",
    "    old_len_fakevocab = len(fake_vocab)\n",
    "    old_len_notfakevocab = len(notfake_vocab)\n",
    "    new_len_fakevocab = 0;\n",
    "    new_len_notfakevocab = 0;\n",
    "    \n",
    "    while(True):\n",
    "        old_len_fakevocab = len(fake_vocab)\n",
    "        old_len_notfakevocab = len(notfake_vocab)\n",
    "        for word in notfake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in notfake_vocab:\n",
    "                                        notfake_vocab.append(lemma.name())\n",
    "\n",
    "\n",
    "\n",
    "        for word in fake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in fake_vocab:\n",
    "                                        fake_vocab.append(lemma.name())\n",
    "        \n",
    "        new_len_fakevocab = len(fake_vocab);\n",
    "        new_len_notfakevocab = len(notfake_vocab);\n",
    "        \n",
    "        if(old_len_fakevocab == new_len_fakevocab and old_len_notfakevocab == new_len_notfakevocab):\n",
    "            break\n",
    "\n",
    "    \n",
    "    test_df[j]['fakescore'] = test_df[j]['text'].apply(getFakenessValue)\n",
    "    \n",
    "    truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "    for i in range(len(test_df[j])):\n",
    "        if list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            truePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] > 0:\n",
    "            trueNegative += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            falsePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] > 0 :\n",
    "            falseNegative += 1\n",
    "\n",
    "    accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "    precision = truePositive/(truePositive + falsePositive)\n",
    "    recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7023151569186739, 0.6622677823783321, 0.7794885203561002)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(accuracies)/len(accuracies),sum(precisions)/len(precisions),sum(recalls)/len(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "\n",
    "for j in range(5):\n",
    "\n",
    "    fake_vocab = {}\n",
    "    notfake_vocab = {}\n",
    "    for i in range(len(train_df[j])):\n",
    "        review = list(train_df[j]['text'])[i]\n",
    "        words = word_tokenize(review)\n",
    "        for word in words:\n",
    "            if word.lower() not in stop_words and len(word) != 1:\n",
    "                if list(train_df[j]['deceptive'])[i] == \"truthful\":\n",
    "                    if lemmatizer.lemmatize(word.lower()) not in notfake_vocab:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                    else:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                elif lemmatizer.lemmatize(word.lower()) not in fake_vocab:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "                else:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "    \n",
    "    \n",
    "    fake_vocab = sorted(fake_vocab.items(), key = \n",
    "             lambda kv:(-kv[1], kv[0]))   \n",
    "    notfake_vocab =sorted(notfake_vocab.items(), key = \n",
    "                 lambda kv:(-kv[1], kv[0]))  \n",
    "    finalFakeVocab = []\n",
    "    finalNotFakeVocab = []\n",
    "    for i in range(600):\n",
    "        finalFakeVocab.append(fake_vocab[i][0])\n",
    "        finalNotFakeVocab.append(notfake_vocab[i][0])\n",
    "    fake_vocab = finalFakeVocab\n",
    "    notfake_vocab = finalNotFakeVocab\n",
    "    \n",
    "    old_len_fakevocab = len(fake_vocab)\n",
    "    old_len_notfakevocab = len(notfake_vocab)\n",
    "    new_len_fakevocab = 0;\n",
    "    new_len_notfakevocab = 0;\n",
    "    \n",
    "    while(True):\n",
    "        old_len_fakevocab = len(fake_vocab)\n",
    "        old_len_notfakevocab = len(notfake_vocab)\n",
    "        for word in notfake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in notfake_vocab:\n",
    "                                        notfake_vocab.append(lemma.name())\n",
    "\n",
    "\n",
    "\n",
    "        for word in fake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in fake_vocab:\n",
    "                                        fake_vocab.append(lemma.name())\n",
    "        \n",
    "        new_len_fakevocab = len(fake_vocab);\n",
    "        new_len_notfakevocab = len(notfake_vocab);\n",
    "        \n",
    "        if(old_len_fakevocab == new_len_fakevocab and old_len_notfakevocab == new_len_notfakevocab):\n",
    "            break\n",
    "\n",
    "    \n",
    "    test_df[j]['fakescore'] = test_df[j]['text'].apply(getFakenessValue)\n",
    "    \n",
    "    truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "    for i in range(len(test_df[j])):\n",
    "        if list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            truePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] > 0:\n",
    "            trueNegative += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            falsePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] > 0 :\n",
    "            falseNegative += 1\n",
    "\n",
    "    accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "    precision = truePositive/(truePositive + falsePositive)\n",
    "    recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "\n",
    "for j in range(5):\n",
    "\n",
    "    fake_vocab = {}\n",
    "    notfake_vocab = {}\n",
    "    for i in range(len(train_df[j])):\n",
    "        review = list(train_df[j]['text'])[i]\n",
    "        words = word_tokenize(review)\n",
    "        for word in words:\n",
    "            if word.lower() not in stop_words and len(word) != 1:\n",
    "                if list(train_df[j]['deceptive'])[i] == \"truthful\":\n",
    "                    if lemmatizer.lemmatize(word.lower()) not in notfake_vocab:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                    else:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                elif lemmatizer.lemmatize(word.lower()) not in fake_vocab:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "                else:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "    \n",
    "    \n",
    "    fake_vocab = sorted(fake_vocab.items(), key = \n",
    "             lambda kv:(-kv[1], kv[0]))   \n",
    "    notfake_vocab =sorted(notfake_vocab.items(), key = \n",
    "                 lambda kv:(-kv[1], kv[0]))  \n",
    "    finalFakeVocab = []\n",
    "    finalNotFakeVocab = []\n",
    "    for i in range(200):\n",
    "        finalFakeVocab.append(fake_vocab[i][0])\n",
    "        finalNotFakeVocab.append(notfake_vocab[i][0])\n",
    "    fake_vocab = finalFakeVocab\n",
    "    notfake_vocab = finalNotFakeVocab\n",
    "    \n",
    "    old_len_fakevocab = len(fake_vocab)\n",
    "    old_len_notfakevocab = len(notfake_vocab)\n",
    "    new_len_fakevocab = 0;\n",
    "    new_len_notfakevocab = 0;\n",
    "    \n",
    "    while(True):\n",
    "        old_len_fakevocab = len(fake_vocab)\n",
    "        old_len_notfakevocab = len(notfake_vocab)\n",
    "        for word in notfake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in notfake_vocab:\n",
    "                                        notfake_vocab.append(lemma.name())\n",
    "\n",
    "\n",
    "\n",
    "        for word in fake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in fake_vocab:\n",
    "                                        fake_vocab.append(lemma.name())\n",
    "        \n",
    "        new_len_fakevocab = len(fake_vocab);\n",
    "        new_len_notfakevocab = len(notfake_vocab);\n",
    "        \n",
    "        if(old_len_fakevocab == new_len_fakevocab and old_len_notfakevocab == new_len_notfakevocab):\n",
    "            break\n",
    "\n",
    "    \n",
    "    test_df[j]['fakescore'] = test_df[j]['text'].apply(getFakenessValue)\n",
    "    \n",
    "    truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "    for i in range(len(test_df[j])):\n",
    "        if list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            truePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] > 0:\n",
    "            trueNegative += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            falsePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] > 0 :\n",
    "            falseNegative += 1\n",
    "\n",
    "    accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "    precision = truePositive/(truePositive + falsePositive)\n",
    "    recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7082547579919092, 0.6768825858872582, 0.7545983747196504)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(accuracies)/len(accuracies),sum(precisions)/len(precisions),sum(recalls)/len(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "\n",
    "for j in range(5):\n",
    "\n",
    "    fake_vocab = {}\n",
    "    notfake_vocab = {}\n",
    "    for i in range(len(train_df[j])):\n",
    "        review = list(train_df[j]['text'])[i]\n",
    "        words = word_tokenize(review)\n",
    "        for word in words:\n",
    "            if word.lower() not in stop_words and len(word) != 1:\n",
    "                if list(train_df[j]['deceptive'])[i] == \"truthful\":\n",
    "                    if lemmatizer.lemmatize(word.lower()) not in notfake_vocab:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                    else:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                elif lemmatizer.lemmatize(word.lower()) not in fake_vocab:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "                else:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "    \n",
    "    \n",
    "    fake_vocab = sorted(fake_vocab.items(), key = \n",
    "             lambda kv:(-kv[1], kv[0]))   \n",
    "    notfake_vocab =sorted(notfake_vocab.items(), key = \n",
    "                 lambda kv:(-kv[1], kv[0]))  \n",
    "    finalFakeVocab = []\n",
    "    finalNotFakeVocab = []\n",
    "    for i in range(500):\n",
    "        finalFakeVocab.append(fake_vocab[i][0])\n",
    "        finalNotFakeVocab.append(notfake_vocab[i][0])\n",
    "    fake_vocab = finalFakeVocab\n",
    "    notfake_vocab = finalNotFakeVocab\n",
    "    \n",
    "    old_len_fakevocab = len(fake_vocab)\n",
    "    old_len_notfakevocab = len(notfake_vocab)\n",
    "    new_len_fakevocab = 0;\n",
    "    new_len_notfakevocab = 0;\n",
    "    \n",
    "    while(True):\n",
    "        old_len_fakevocab = len(fake_vocab)\n",
    "        old_len_notfakevocab = len(notfake_vocab)\n",
    "        for word in notfake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in notfake_vocab:\n",
    "                                        notfake_vocab.append(lemma.name())\n",
    "\n",
    "\n",
    "\n",
    "        for word in fake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in fake_vocab:\n",
    "                                        fake_vocab.append(lemma.name())\n",
    "        \n",
    "        new_len_fakevocab = len(fake_vocab);\n",
    "        new_len_notfakevocab = len(notfake_vocab);\n",
    "        \n",
    "        if(old_len_fakevocab == new_len_fakevocab and old_len_notfakevocab == new_len_notfakevocab):\n",
    "            break\n",
    "\n",
    "    \n",
    "    test_df[j]['fakescore'] = test_df[j]['text'].apply(getFakenessValue)\n",
    "    \n",
    "    truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "    for i in range(len(test_df[j])):\n",
    "        if list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            truePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] > 0:\n",
    "            trueNegative += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            falsePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] > 0 :\n",
    "            falseNegative += 1\n",
    "\n",
    "    accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "    precision = truePositive/(truePositive + falsePositive)\n",
    "    recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6952928218432929, 0.66137457491443, 0.7918397566996694)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(accuracies)/len(accuracies),sum(precisions)/len(precisions),sum(recalls)/len(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = [0]*5\n",
    "test_df = [0]*5\n",
    "for i in range(5):\n",
    "    train_df[i],test_df[i] = train_test_split(dataset,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "\n",
    "for j in range(5):\n",
    "\n",
    "    fake_vocab = {}\n",
    "    notfake_vocab = {}\n",
    "    for i in range(len(train_df[j])):\n",
    "        review = list(train_df[j]['text'])[i]\n",
    "        words = word_tokenize(review)\n",
    "        for word in words:\n",
    "            if word.lower() not in stop_words and len(word) != 1:\n",
    "                if list(train_df[j]['deceptive'])[i] == \"truthful\":\n",
    "                    if lemmatizer.lemmatize(word.lower()) not in notfake_vocab:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                    else:\n",
    "                        notfake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                         for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in notfake_vocab:\n",
    "#                                     notfake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     notfake_vocab[lemma.name()] += 1\n",
    "                elif lemmatizer.lemmatize(word.lower()) not in fake_vocab:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] = 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "                else:\n",
    "                    fake_vocab[lemmatizer.lemmatize(word.lower())] += 1\n",
    "#                     for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "#                             for lemma in synset.lemmas():\n",
    "#                                 if lemma.name() not in fake_vocab:\n",
    "#                                     fake_vocab[lemma.name()] = 1\n",
    "#                                 else:\n",
    "#                                     fake_vocab[lemma.name()] += 1\n",
    "    \n",
    "    \n",
    "    fake_vocab = sorted(fake_vocab.items(), key = \n",
    "             lambda kv:(-kv[1], kv[0]))   \n",
    "    notfake_vocab =sorted(notfake_vocab.items(), key = \n",
    "                 lambda kv:(-kv[1], kv[0]))  \n",
    "    finalFakeVocab = []\n",
    "    finalNotFakeVocab = []\n",
    "    for i in range(500):\n",
    "        finalFakeVocab.append(fake_vocab[i][0])\n",
    "        finalNotFakeVocab.append(notfake_vocab[i][0])\n",
    "    fake_vocab = finalFakeVocab\n",
    "    notfake_vocab = finalNotFakeVocab\n",
    "    \n",
    "    old_len_fakevocab = len(fake_vocab)\n",
    "    old_len_notfakevocab = len(notfake_vocab)\n",
    "    new_len_fakevocab = 0;\n",
    "    new_len_notfakevocab = 0;\n",
    "    \n",
    "    while(True):\n",
    "        old_len_fakevocab = len(fake_vocab)\n",
    "        old_len_notfakevocab = len(notfake_vocab)\n",
    "        for word in notfake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in notfake_vocab:\n",
    "                                        notfake_vocab.append(lemma.name())\n",
    "\n",
    "\n",
    "\n",
    "        for word in fake_vocab:\n",
    "            for synset in wn.synsets(lemmatizer.lemmatize(word.lower())):\n",
    "                                for lemma in synset.lemmas():\n",
    "                                    if lemma.name() not in fake_vocab:\n",
    "                                        fake_vocab.append(lemma.name())\n",
    "        \n",
    "        new_len_fakevocab = len(fake_vocab);\n",
    "        new_len_notfakevocab = len(notfake_vocab);\n",
    "        \n",
    "        if(old_len_fakevocab == new_len_fakevocab and old_len_notfakevocab == new_len_notfakevocab):\n",
    "            break\n",
    "\n",
    "    \n",
    "    test_df[j]['fakescore'] = test_df[j]['text'].apply(getFakenessValue)\n",
    "    \n",
    "    truePositive = trueNegative = falsePositive = falseNegative = 0\n",
    "    for i in range(len(test_df[j])):\n",
    "        if list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            truePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] > 0:\n",
    "            trueNegative += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'truthful' and list(test_df[j]['fakescore'])[i] < 0:\n",
    "            falsePositive += 1\n",
    "        elif list(test_df[j]['deceptive'])[i] == 'deceptive' and list(test_df[j]['fakescore'])[i] > 0 :\n",
    "            falseNegative += 1\n",
    "\n",
    "    accuracy = (truePositive + trueNegative)/(truePositive + trueNegative + falsePositive + falseNegative)\n",
    "    precision = truePositive/(truePositive + falsePositive)\n",
    "    recall =truePositive/(truePositive + falseNegative) \n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6969938963477029, 0.7112625243585258, 0.6457704794706511)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(accuracies)/len(accuracies),sum(precisions)/len(precisions),sum(recalls)/len(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7028301886792453,\n",
       " 0.7285067873303167,\n",
       " 0.6651376146788991,\n",
       " 0.6153846153846154,\n",
       " 0.6333333333333333]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
